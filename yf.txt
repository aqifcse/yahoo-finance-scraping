# from lxml import html
# from requests_html import HTMLSession
# import json
# # import argparse
# from collections import OrderedDict

# # def get_search_url(query_keyword):
# #     """
# #     parse html page form url
# #     :param text:
# #     :return:
# #     keyword': elements from the queries
# #     """

# #     base_url = 'https://www.amazon.in/s?k='
     
# #     session = HTMLSession()

# #     request_url = base_url + query_keyword
# #     print(request_url)
# #     resp = session.get(request_url)
# #     soup = BeautifulSoup(resp.text, "lxml")
# #     return soup

# def parse(ticker):
#     url = "http://finance.yahoo.com/quote/%s?p=%s" % (ticker, ticker)
#     session = HTMLSession()
#     response = session.get(url)
#     print("Parsing %s" % (url))
#     parser = html.fromstring(response.text)
#     summary_table = parser.xpath(
#         '//div[contains(@data-test,"summary-table")]//tr')
#     summary_data = OrderedDict()
#     other_details_json_link = "https://query2.finance.yahoo.com/v10/finance/quoteSummary/{0}?formatted=true&lang=en-US&region=US&modules=summaryProfile%2CfinancialData%2CrecommendationTrend%2CupgradeDowngradeHistory%2Cearnings%2CdefaultKeyStatistics%2CcalendarEvents&corsDomain=finance.yahoo.com".format(
#         ticker)
#     summary_json_response = session.get(other_details_json_link)
#     try:
#         json_loaded_summary = json.loads(summary_json_response.text)
#         summary = json_loaded_summary["quoteSummary"]["result"][0]
#         y_Target_Est = summary["financialData"]["targetMeanPrice"]['raw']
#         earnings_list = summary["calendarEvents"]['earnings']
#         eps = summary["defaultKeyStatistics"]["trailingEps"]['raw']
#         datelist = []

#         for i in earnings_list['earningsDate']:
#             datelist.append(i['fmt'])
#         earnings_date = ' to '.join(datelist)

#         for table_data in summary_table:
#             raw_table_key = table_data.xpath(
#                 './/td[1]//text()')
#             raw_table_value = table_data.xpath(
#                 './/td[2]//text()')
#             table_key = ''.join(raw_table_key).strip()
#             table_value = ''.join(raw_table_value).strip()
#             summary_data.update({table_key: table_value})
#         summary_data.update({'1y Target Est': y_Target_Est, 'EPS (TTM)': eps,
#                              'Earnings Date': earnings_date, 'ticker': ticker,
#                              'url': url})
#         return summary_data
#     except ValueError:
#         print("Failed to parse json response")
#         return {"error": "Failed to parse json response"}
#     except:
#         return {"error": "Unhandled Error"}


# if __name__ == "__main__":
#     # argparser = argparse.ArgumentParser()
#     # argparser.add_argument('ticker', help='')
#     # args = argparser.parse_args()
#     #ticker = args.ticker

#     ticker = 'JPXN'
#     #print("Fetching data for %s" % (ticker))
#     scraped_data = parse(ticker)
#     print("Writing data to output file")
#     with open('%s-summary.json' % (ticker), 'w') as fp:
#         json.dump(scraped_data, fp, indent=4)

from bs4 import BeautifulSoup
from requests_html import HTMLSession
from urllib.parse import urljoin
import json

ticker = 'JPXN'


def get_search_url(query_keyword):
    """
    parse html page form url
    :param text:
    :return:
    keyword': elements from the queries
    """

    request_url = "http://finance.yahoo.com/quote/%s?p=%s" % (ticker, ticker)
     
    session = HTMLSession()

    #request_url = base_url + query_keyword
    print(request_url)
    resp = session.get(request_url)
    soup = BeautifulSoup(resp.text, "lxml")
    return soup

# def feature_product_details(url, query_keyword):
    
    
#     # file = open('_output.json', 'w', encoding='utf-8')
#     # json.dump(query_output, file, ensure_ascii=False)

#     with open('%s-summary.json' % (ticker), 'w') as fp:
#         json.dump(scraped_data, fp, indent=4)

#     product_lists = []

#     return product_lists
def parse(url, query_keyword):
    

    #find_all("div", attrs={"class":"sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 sg-col sg-col-4-of-20"})
    summary_table = url.find_all("div",attrs={"data-test":"summary-table"})
    summary_data = OrderedDict()
    other_details_json_link = "https://query2.finance.yahoo.com/v10/finance/quoteSummary/{0}?formatted=true&lang=en-US&region=US&modules=summaryProfile%2CfinancialData%2CrecommendationTrend%2CupgradeDowngradeHistory%2Cearnings%2CdefaultKeyStatistics%2CcalendarEvents&corsDomain=finance.yahoo.com".format(
        ticker)
    summary_json_response = session.get(other_details_json_link)
    try:
        json_loaded_summary = json.loads(summary_json_response.text)
        summary = json_loaded_summary["quoteSummary"]["result"][0]
        y_Target_Est = summary["financialData"]["targetMeanPrice"]['raw']
        earnings_list = summary["calendarEvents"]['earnings']
        eps = summary["defaultKeyStatistics"]["trailingEps"]['raw']
        datelist = []

        for i in earnings_list['earningsDate']:
            datelist.append(i['fmt'])
        earnings_date = ' to '.join(datelist)

        for table_data in summary_table:
            raw_table_key = table_data.xpath(
                './/td[1]//text()')
            raw_table_value = table_data.xpath(
                './/td[2]//text()')
            table_key = ''.join(raw_table_key).strip()
            table_value = ''.join(raw_table_value).strip()
            summary_data.update({table_key: table_value})
        summary_data.update({'1y Target Est': y_Target_Est, 'EPS (TTM)': eps,
                             'Earnings Date': earnings_date, 'ticker': ticker,
                             'url': url})
        
        return summary_data
    except ValueError:
        print("Failed to parse json response")
        return {"error": "Failed to parse json response"}
    except:
        return {"error": "Unhandled Error"}

def main_fun_2(query):
    """
    main function for interact with this module
    :param text: user input
    :return: product_title, product_price
    """
    query_keyword = query
    page = get_search_url(query_keyword)
    scraped_data = parse(page, query_keyword)


    with open('%s-summary.json' % (ticker), 'w') as fp:
        json.dump(scraped_data, fp, indent=4)

    return scraped_data


if __name__ == '__main__':
    
    main_fun_2(ticker)
